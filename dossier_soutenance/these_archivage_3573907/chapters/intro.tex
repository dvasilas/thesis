% \section{Background and motivation}

Today's global-scale services handle large volumes of data and serve large volumes of requests from users distributed worldwide.
They rely on large-scale data serving systems which distribute data across multiple geographically distant data centers
in order to reduce response time and tolerate failures.
Moreover, services increasingly spread their data between on-premise and remote cloud nodes,
as well as between multiple cloud providers, in order to increase reliability and reduce costs.

An important aspect of the data systems that developers rely on for building applications is a powerful query language.
Google, describing the evolution of Spanner from a key-value store to a relational database \cite{corbett:spanner},
has reported that Google developers found it difficult to build applications on a system lacking a strong schema system
and a robust query language.
Query processing is an essential component of data systems' technology, as its role is to bridge declarative query languages,
in which queries specify the patterns and conditions that results must meet, and efficient execution.

In this thesis, we study the challenges of supporting efficient query processing in contexts in
which users and data are distributed across multiple geographic locations.
In particular, we examine the design decisions and trade-offs involved in the design of a query engine over geo-distributed
data, which serves users distributed worldwide.
We consider a query engine that implement operators for performing transformations over a corpus (selections, aggregations, joins)
and, in addition, employs techniques for accelerating query processing.
These techniques commonly involve the materialization of \textit{derived state}.
This includes maintaining copies of the data organized in forms that facilitate different access patterns (indexes),
and pre-computing the results of expensive recurring operations (materialized views).

The design of such a query engine must meet multiple requirements.
Applications expect query engines to serve requests in a timely fashion and to provide accurate results.
Moreover, query processing must incur low overhead to other aspects of data systems' operation and to their operational cost.

% It is well known that
Query processing in a geo-distributed context involves several challenges.
In geo-distributed deployments,
latencies between data centers can be orders of magnitude higher that those within a data center.
Moreover, network resources between data centers are often limited and costly.
Because of these factors,
in geo-distributed query processing
the requirements of low response time, accurate query results and low overhead to the system's performance and operational
cost are incompatible and cannot be achieved all at once:
% incompatible and thus query engines must balance trade-offs between desirable properties.

\begin{itemize}
  \item User-perceived query response time is composed of two components:
  query network time, which accounts for the time spent to send a query from the user to the query engine over the network
  and the time spent to send search results back to the user, and query processing time.
  Techniques for reducing query processing time, like the above-mentioned, have been extensively studied in the context of database systems.
  However, in geo-distributed scenarios, query network time becomes a significant factor to query response time.

  \item At the same time, the derived state structures that query engines use to accelerate query processing must be kept up-to-date with changes to the corpus.
  This requires propagating and applying the effects of write operations to the derived state.
  Because of that, maintaining indexes and materialized views incurs overhead to the latency of write operations.
  An approach employed by query engines to address this issue is to perform this maintenance task asynchronously,
  outside of the critical path of write operations.
  This, however, entails that derived state is only eventually consistent, and can be temporarily stale relative to the corpus.
  Serving queries from stale indexes or materialized views may result in query results not being consistent with the state of the corpus.
\end{itemize}

Query network time can be reduced by replicating the query engine's derived state on every data center,
thus ensuring that all queries can be served from the local data center without requiring communication with other data centers.
However, maintaining a full replica of every index and materialized view on every data center incurs significant memory and storage overhead.
Moreover, to remain up-to-date with the corpus, a derived state replica must receive the effects of write operation from every data center.
Assuming asynchronous maintenance, this means that derived state can become significantly stale, affecting the accuracy of query results.
In addition, in write-dominated workloads, this results in significant data transfer between data centers for state maintenance.

At the other end of the design space,
derived state can be partitioned into non-overlapping parts and each part assigned to a data center
This approach improves derived state freshness.
However, every query needs to be forwarded to every part of the derived state, resulting in significant overhead in query network time.
Moreover, in query-dominated workloads this increases data transfer between data centers for query processing.

It becomes apparent that, in this setting, design decisions and about the distribution and placement of derive state
and the communication patterns between the corpus, the users and the derived state,
result in trade-offs between query response time, query result consistency, and the system's operational cost.
Techniques such as caching of query results add additional points in this design space.

At the same time,
modern applications have diverse query processing requirements.
User-facing queries have strict response time requirements, as response times directly affect revenues.
On the other hand, query engines that serve social media applications are required to ingest new content rapidly \cite{busch:earlybird}.

In addition to that, different design decisions about derived state distribution and placement are better suited to different workload characteristics.
A design that requires only local communication between the corpus and derived state is better suited for write-dominated workloads in terms of data transfer costs.
Conversely, a design that requires only local communication for query processing is more cost-effective for query-dominated workloads.

It therefore becomes evident that no single query engine design can be suitable for all needs.
However, existing query engines are designed to be general-purpose systems;
They aim at handling a variety of use cases and workloads.
In the context of geo-distribution, general-purpose query engines are often inefficient for certain workloads
and application requirements.
Specialized solutions can be much more efficient,
but adjusting derived state distribution and placement in current query engine architectures can be time and resource consuming.

In this thesis, we make the case for a \textit{flexible} query engine architecture that can be adjusted to the requirements
and characteristics of different applications in a case-by-case basis, with low engineering effort.

\section{Contributions}

Traditional monolithic query engine architectures do not provide the flexibility required to adjust the trade-offs
between query response time, query result consistency, and operational cost according to the characteristics and
requirements of different geo-distributed applications.

To address this challenge, we propose a new \textit{modular} query engine architecture.
Our key insight is that query processing can be decomposed into a set primitives,
such as selecting from a set of data items the ones that satisfy a given condition,
or constructing from a set of data items a secondary index on a given attribute.
Each primitive can be encapsulated by an independent component that interoperates with other components through a set of interfaces.
We show that, while different components encapsulate different query processing primitives,
all components can expose a common set of interfaces with common semantics.

Guided by this insight,
we introduce an architecture component abstraction, called Query Processing Unit (QPU).
The QPU abstraction defines the properties, interfaces and semantics of a generic query engine component.
In order to express different types of query processing primitives,
the QPU abstraction combines the properties of a streaming operator and a microservice.
Akin to a microservice, a QPU component maintains internal state, is configured independently, and exposes its functionality to other components through a service interface;
Akin to a streaming operator, a QPU component operates by receiving one or more input streams from other components,
performs a computation over these streams, and emits an output stream.

Different instantiations of the QPU abstraction (QPU classes) implement different query processing primitives.
These include relational operators, such as selections, aggregations, and joins,
derived state structures, such as indexes, materialized views and caches,
and ``routing'' functionalities, such as load balancing, and managing index partitions.

Query Processing Units are aimed to be used as building blocks for constructing query engine architectures.
A query engine is a directed acyclic graph (DAG), with QPUs as vertices;
Graph edges indicate communication paths between QPUs.
An edge from $QPU_A$ to $QPU_B$ indicates that $QPU_A$ can invoke the interface of $QPU_B$,
and that, as a result, it can receive an input stream from $QPU_B$.
By specifying the QPU class and configuration of each graph vertex, as well as the graph topology,
one can control how query engine's derived state is partitioned,
and the communication patterns required for query processing and state maintenance.
Because the operation of a QPU depends only on its interaction with other QPUs and not its placement,
each QPU can be strategically placed across the system.

A QPU DAG runs a bidirectional data-flow computation.
The corpus is situated at the leaf nodes of the graph, while queries enter the graph through root nodes.
Updates to the corpus stream ``upwards''
\footnote{upwards and downwards are our representational convention} through the query engine graph, and incrementally update the QPUsâ€™ internal data structures;
Conversely, queries stream ``downwards'', are incrementally transformed in sub-queries which are processed in different parts of the graph.
Partial results are then incrementally combined (flowing back upward) to produce the query response.

\medskip
\noindent
We realize this architecture model in the form of a query processing framework, called Proteus.
Proteus provides a library of Query Processing Unit implementations,
a service discovery mechanism that allows QPU graphs to self-organize with only local configuration to each QPU,
and a configuration language for specifying query engine architectures.

Proteus aims at enabling developers to design and deploy query engines in a case-by-case basis,
by enabling flexibility over the distribution and placement of the query engine's state and computations.

% In its current form, the architectural model is not focused on general purpose query engine that can execute an ad-hoc query.
% instead, a query engine is constructed and deployed with a specific query workload (set of queries) in mind.

In summary, this work makes the following contributions:

\begin{itemize}
  \item A comprehensive analysis of the design choices and corresponding trade-offs of geo-distributed query processing.

  \item A novel modular query engine architectural model, based on the Query Processing Unit abstraction,
  which enables flexible distribution and placement of query processing state and computations.

  \item Case studies that demonstrate how a flexible query engines architectures can be applied to a number of applications
  and provide improvements over state-of-the-art approaches.

  \item Proteus, a framework for constructing and deploying query engines using the proposed architectural model.

  \item An experimental evaluation that confirms the theoretic trade-off analysis,
  by demonstrating the benefits and drawbacks of different and placement schemes,
  and shows that Proteus can efficiently implement different points in the design space of query processing state placement.
\end{itemize}

Contributions of the thesis are presented in \cite{10.1145/3194261.3194265} and \cite{app:rep:sh222}.


\section{Thesis outline}

The rest of this thesis is organized as follows.
Chapter \ref{ch:models} introduces the system and data model that we consider in this work,
and discusses the requirements for an efficient and effective query engine design.
Chapter \ref{ch:background} provides an overview of concepts related to the work presented in this thesis.
It provides a broad overview of query processing in database systems,
focusing on techniques for facilitating query processing by maintaining derived state (indexes, materialized views and caches).
Chapter \ref{ch:design_space} presents an analysis of the design decisions and trade-offs involved in the design
of query engines that derived state.
Chapter \ref{ch:design_pattern} presents the specification of the Query Processing Unit abstraction,
and the proposed query engine architecture model.
Chapter \ref{ch:case_studies} demonstrates the proposed architecture's expressiveness by applying it to a number
of use cases and applications showing how it can provide improvements over the state-of-the-art.
Chapter \ref{ch:proteus} describes the Proteus framework.
Chapter \ref{ch:evaluation} evaluates the proposed approach,
and shows how flexible state and computation placement can allow applications to balance the trade-offs between
query response time, result accuracy and operational cost.
Chapter \ref{ch:related_work} overviews related work.
Chapter \ref{ch:future} outlines directions for future work.
Finally, Chapter \ref{ch:conclusion} concludes the thesis.
