\section{Query Processing in Relational Database Systems}

\section{Query Processing in NoSQL Datastores}

\subsection{NoSQL Datastores}

\subsubsection{NoSQL Data models}

\subsubsection{Partitioning}
\label{sec:partitioning}

Partitioning is data distribution technique, in which a database's records are split into subsets called partitions so
that different partitions can be assigned to different nodes (also known as sharding).

The goal of partitioning is to spread data and load evenly across nodes.
When implemented efficiently, it enables horizontal scaling:
doubling the number of nodes in the system should make the system able to handle double the volume of data, and
should double the system's read and write throughput.

The sharding techniques commonly used in NoSQL systems are:

\bigskip
\noindent
\textbf{Range partitioning.}
Range partitioning assigns a continuous range of keys to each partition.
These ranges of key are not necessarily evenly spaced, because data may not be evenly distributed.
Partition boundaries might be chosen manually by an administrator, or the database can choose them automatically.

Within each partition keys are kept in sorted order.
This has the advantage that range queries on the partitioning key are efficient:
it is easy to determine which partitions contain keys of a given range, and within each partition the key can be treated
as an index.

The downside of this partitioning scheme is that certain access patterns can lead to hotspots.
Therefore, systems that implement range partitioning need mechanisms for detecting and resolving hotspots, by splitting
overburdened shards.

This partitioning strategy is used by Bigtable, its open source equivalent HBase \cite{hbasebigtable:comparison}, RethinkDB,
and MongoDB before version 2.4.

\bigskip
\noindent
\textbf{Hash partitioning.}
An alternative approach that avoids the risks of skew and hotspots is to use a hash function to determine the partition
for a given key.
Hash partitioning assigns each partition a range of hashes --- rather than a range of keys --- and every key whose has
falls within a partition's range is handled by that partition.

This partitioning scheme is efficient at distributing keys fairly among partitions.
However, this approach does not allow for efficient range queries, as adjacent keys are scattered across multiple
partitions.

Hash partitioning is used in Amazon's Dynamo, MongoDB since 2.4 \cite{mongo:hashpartitioning}, Riak, CouchBase,
and Voldemort.

\subsubsection{Replication}

Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes.
Replication increases availability (allowing the systems to continue working even if some of its parts have failed)
To  (and thus increase availability)
To scale out the number of machines that can serve read queries (and thus increase read throughput)

Considering a single partition, each node that takes part in the replication, called a replica, stores a copy of the
partition.
Each write to the database needs to be processed by every replica;
A replication strategy answers two questions: where a write is accepted, and when the corresponding data changes are
propagated to replicas
TODO: cite The dangers of replication and a solution.

\bigskip
\noindent
The most common approach to the first question this is called \textit{leader-based} replication.
One of the replicas is designated the \textit{leader} (also termed \textit{master} or \textit{primary}).
Database writes are sent to the leader.
The leader determines the order in which writes should be processed, sends the corresponding data changes to the other
replicas (termed \textit{followers}, \textit{slaves} or \textit{secondaries}),
and followers apply those changes in the same order.
Databases reads can be performed either from the leader or any of the followers.

This mode of replication is used in MongoDB, RethinkDB, and Espresso.

Leader-based replication has one main downside: as there is only one reader (when replication is combined with partitioning there is one leader per partition),
and all database writes must go through it; if the leader is unreachable writes cannot be performed.

An extension of leader-based replication is to allow more than one replicas to accepts writes.
In \textit{multi-leader} replication, there are multiple readers, each processing writes and forwarding the corresponding data changes to all other nodes;
each leader simultaneously acts as a follower to the other leaders, accepting writes from them.

An alternative approach, termed \textit{leaderless} replication, is to allow any replica directly accept writes from clients.
Each write is sent (either by the client, or by a coordinator) to $W$ replicas, and each read is sent to $R$ replicas,
where $W$ and $R$ are per-operation parameters.
In order to ensure that eventually all data is copied to every replica, leaderless replication implementations often employ two mechanisms:
(1) read repair, detecting and updating stale values during read, and (2) anti-entropy, having a background process that replicates missing data between replicas.
This approach was popularized by Amazon's Dynamo.
Riak, Cassandra, and Voldemort are datastores with leader replication models inspired by Dynamo.

\bigskip
\noindent
There are two approaches for the question ``when the leader propagates data changes to followers''.
In \textit{synchronous} (or \textit{eager}) replication the leader propagates changes synchronously and waits for acknowledgements from followers before reporting
success to the user.
In \textit{asynchronous} (or \textit{lazy}) replication the leader propagates changes and does not wait for responses from the followers.

The advantage of synchronous replication is that followers are guaranteed to have copies of the data that are up-to-date with the leader.
The disadvantage is that if followers do not respond (due to a crash, network fault or other reasons) writes cannot be processed.

\bigskip
\noindent
Geo-replication (replication across geographically distributed data centers) can protect the system against data center failures and network problems,
and improve read latency for clients distributing across multiple geographic locations.
Synchronous geo-replication, as implemented in Google's Megastore \cite{baker:megastore} and Spanner \cite{corbett:spanner, bacon:spanner},
achieves strong consistency at the cost of high write latency.
In asynchronous geo-replication, as used in Dynamo \cite{deCandia:dynamo}, PNUTS \cite{cooper:pnuts08, cooper:pnuts19},
Walter \cite{sovran:walter}, COPS \cite{lloyd:cops}, Cassandra \cite{lakshman:cassandra}, and Bigtable \cite{chang:bigtable}
the inter-data center network delays are hided from clients, and the system remains available during partitions.
The downside for the approach is that the same data may be concurrently modified in different data centers creating
conflicts that then need to be resolved.


\subsection{Query Processing}
\subsubsection{Secondary indexes}

A common technique used to support efficient filter queries the use of secondary indexes (secondary indexing).

A secondary index is an additional structure that is derived from the primary data, and stores data in a form that
provides a way to efficiently access records in a database by means other than the primary key.

Essentially, a secondary index is a key-value structure where the key is a \textit{term} (an attribute or key of the
database record other than the primary key) and the
value is a list of primary keys of all the records that contain that term (a \textit{posting list}).

Secondary indexing is an instance of a general system design pattern:
having the same data represented in different formats to address different access patterns.
Database tables are the primary copy of data.
Derived copies of the data transform or represent the primary copy differently in order to satisfy certain access patterns.
Adding a secondary index does not affect the contents of the database; it only affects the performance of (read and write)
queries.
Writes go to the primary data and all of the other data copies are derived from it.
The other copies only serve read requests.
TODO: address that some systems support writes to a view.

The following data structures are commonly used for implementing secondary indexes:

\medskip
\noindent
\textbf{B-Trees}

\medskip
\noindent
\textbf{LSM-Trees}

\bigskip

\noindent
TODO:
In this work, ...

\subsubsection{Partitioning and Secondary Indexes}

The partitioning schemes discussed in \ref{sec:partitioning} rely on a key-value data model.
Secondary indexes do not neatly map to the partitioning technique:
a secondary index usually does not uniquely identify a data item, but rather provides a way of searching for occurrences
of a particular value.

There are two main approaches to partitioning a secondary index:
document-based partitioning and term-based partitioning.

The terminology used in the rest of this section comes from the literature of full-text indexes (a particular kind of secondary index):
a document is a self-contained piece of information, that is composed of terms.

\bigskip

\noindent
\textbf{Partitioning Indexes by Document.}
In this approach, each partition is separate:
each partition maintains its own secondary indexes, covering only documents in that partition.
A document-partitioned index is also known as a \textit{local index}.

When this approach is used, each database write (adding, removing, or updating a document) is handled only by the
partition that contains the corresponding document.
However, reading from a document-partitioned index requires a scatter/gather approach:
sending the query to all partitions, and combining the returned results.
This can make index lookups quite expensive.
Even if index lookup requests are sent to partitions in parallel, scatter/gather is prone to tail latency amplification,
as the total latency depends from the latency of the slowest index partition.

This approach is widely used: MongoDB, Riak \cite{riakv:secondaryindexes}, Cassandra \cite{cassandra:secondaryindexing}
Elasticsearch \cite{elastic:docrouting}, Solr \cite{solr:indexsharding}.

\bigskip

\noindent
\textbf{Partitioning Secondary Indexes by Term.}
An alternative approach is to construct a \textit{global index} that covers data in all partitions.
A global index, however, also needs to be partitioned, as storing it on one node would likely become a bottleneck.

To partition a global index, the indexed terms can be used as the partition key (thus the term \textit{term-partitioned}
index).
Same as in base data partitioning, the index partitioning scheme can use the terms themselves, which can be useful for
range scans, or a the terms' hashes, which results to a more even load distribution.

The advantage of a term-partitioned index is that it can make reads more efficient:
rather than requiring a scatter/gather over all partitions, a lookup for a given term only needs to make a request to the
partition containing that term.
The downside of this approach is that writes are more complicated and slower:
a write to a single document may affect multiple partitions, the document's terms may be on different partitions.

DynamoDB supports both global and local secondary indexes \cite{dynamodb:secondaryindexes}.
Global indexing has also been used in the research systems such as SLIK \cite{kejriwal:slik} and Diff-Index \cite{tan:diffindex}.

\bigskip
TODO: discuss routing on the two approaches.

TODO:
% https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html
% When you put or delete items in a table, the global secondary indexes on that table are updated in an eventually consistent fashion. Changes to the table data are propagated to the global secondary indexes within a fraction of a second, under normal conditions. However, in some unlikely failure scenarios, longer propagation delays might occur. Because of this, your applications need to anticipate and handle situations where a query on a global secondary index returns results that are not up to date.

\bibliographystyle{plainnat}
\bibliography{refs}