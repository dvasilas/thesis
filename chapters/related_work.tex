In this chapter, we discuss literature related to varying aspects of this work.
After presenting secondary indexing approaches in distributed data stores in Section~\ref{sec:secondaryindexing},
we discuss approaches for maintaining materialized view as a tier on top of base data in Section~\ref{sec:mv},
and caching in Section~\ref{sec:related_caching}.
This work builds on top of these techniques by studying the design decisions and trade-offs that result from applying
them in a geo-distributed context.
In Section~\ref{sec:federation} we summarize works addressing the problem of query processing over geo-distributed corpora.
Then, in Sections~\ref{sec:modular_arch} and~\ref{sec:placement}
we present related work in the areas of state and computation placement, and modular architectures respectively.
Finally, in Section~\ref{sec:dataflowmapreduce} we discuss the relation between the query processing unit computation model
and MapReduce and dataflow execution models.


\section{Secondary indexing in distributed data stores}
\label{sec:secondaryindexing}

A number of works \cite{kejriwal:slik, dsilva:tworings, tan:diffindex, tang:deferredindexing}
have examined the challenges of implementing secondary indexing in distributed databases have been proposed.

SLIK \cite{kejriwal:slik} discusses the design space related to several of aspects of designing a secondary indexing system
for a distributed key-value store, including index partitioning, index -- base data consistency,
and performing long-running operations such as index creation and migration.

SLIK partitions secondary indexes using the partitioning scheme that we refer to as partitioning by term.
Index partition are implemented using B+trees.
Index entries store hashes of data items' primary keys and therefore,
following an index lookup, each data item in the lookup result needs to be retrieved from the storage engine.

To achieve consistent index lookups SLIK uses two techniques.
First, index entries are created before updates are applied to the corresponding base data items,
and old index entries are removed asynchronously.
This ensures that the lifespan of an index entry spans that of the corresponding base data item,
and thus guarantees that if a data item contains a secondary attribute value, then an index lookup for that value will
return the data item.
Second, the query engine removes false positives by verifying index lookup results with the corresponding base data items.
In other words, base data is treated as the ground truth and index entries as hints.

Moreover, SLIK performs long-running bulk operations such as index creation, deletion and migration in the background,
in order to avoid blocking blocking normal operations.

While this work discusses alternative approaches to several aspects of secondary indexing,
SLIK's design make is based on certain requirements and as a result makes specific design decisions and trade-offs.
For instance, SLIK aims at providing the same strong consistency for index lookups as a centralized system.
Conversely, this works aims at providing flexibility, by proposing a system architecture that can express multiple
different point in the design space of secondary indexing.

\bigskip
\noindent
D'Silva et al. \cite{dsilva:tworings} examine two approaches for implementing partitioned secondary indexes over a
distributed data store:
One is makes use of the underlying data store, by implementing secondary indexes as ``system '' tables in the underlying
data store.
In the second approach, each table partition is responsible for maintaining its portion of the secondary index,
and therefore index entries are co-located with the corresponding base table records.
Both approaches have been implemented in HBase\footnote{https://hbase.apache.org/}.

Experimental comparison of the two approaches shows that there is no one-size-fits-all solution secondary indexing in
this setting.
Rather, each approach is better suited for different needs,
based on factors such as the distribution of values of the indexed attribute,
the size of the system (number of partitions), the amount of concurrency in index lookups,
and the selectivity of queries.
We have described the experimental results of \cite{dsilva:tworings} in more detail in Section~\ref{sec:index_partitioning_design_space}.
Both strategies are implemented into HBase.

While other works \cite{kejriwal:slik, tan:diffindex} have discussed the design choices and trade-offs related to secondary
indexing in distributed data stores, the work of D'Silva et al. is, in our knowledge, the first to experimentally
demonstrate that no single approach to secondary indexing can be best for all cases.
However, this work considers the environment of a cluster of servers and a synchronous maintenance scheme.
Our work expands on the work of D'Silva et al. in two directions.
First, we consider the design decisions involved in secondary indexing (and query processing using derived state in general)
in the context of geo-distribution.
Second, we propose a framework that application can use to navigate this design space,
making trade-offs according to their needs.

%  // Replex
% no need to compromise scalability for functionality.
% We present Replex, a data store that enables efficient querying on multiple keys by rethinking data placement during replication
% multi-key datastore.
% Traditionally, a data store is first globally partitioned, then each partition is replicated identically to multiple nodes
% Replex relies on a novel replication unit, termed replex, which partitions a full copy of the data based on its unique key.
% Several NoSQL datastores have emerged that can support queries on multiple keys through the use of secondary indexes.
% In Replex, each full copy of the data may be partitioned by a different key,
% thereby retaining the ability to support queries against multiple keys without incurring a performance penalty
% or storage overhead beyond what is required to protect the database against failure.
% which makes use of a novel replication unit we call a replex. The key insight of a replex is to combine the need to replicate for fault- tolerance and the need to replicate for index availability.
% A replex stores a table and shards the rows across multiple partitions. All replexes store the same data (ev- ery row in the table), the only difference across replexes is the way data is partitioned and sorted, which is by the sorting key of the index associated with the replex.
% uses chain replication to replicate a row to a number of replex partitions, each of which sorts the row by the replex’s corresponding index
% Programmers need to be able to query data by more than just a single key. For many NoSQL systems, supporting multiple indexes is more of an afterthought: a reaction to programmer frustration with the weakness of the NoSQL model.
% Replex reconsiders multi-index data stores from the bottom-up, showing that implementing secondary in- dexes can be inexpensive if treated as a first-order concern.
% novel replication scheme which considers fault- tolerance, availability, and indexing simultaneously.

% In this paper, we explore the challenges associated with indexing modern distributed table-based data stores and investigate two secondary index approaches which we have integrated within HBase.
% Our detailed analysis and experimental results prove the benefits of both the approaches.
% Further, we demonstrate that such secondary index implementation decisions cannot be made in isolation of the data distribution and that different indexing approaches can cater to different needs.
% We discuss two indexing strategies for distributed key-value stores:
% one based on distributed tables that is able to exploit the table model of the underlying system for index management,
% the other using a co-location approach allowing for efficient main-memory access

\subsection{Consistency between index and base data}

In traditional database management systems, search is a primary mechanism for data retrieval.
In addition, indexes are used internally for other operations such as view maintenance.
Due to these reasons, traditional database management systems keep indexes always up-to-date with base tables by performing
index maintenance synchronously, in the critical path of write operations.
On the other hand, in the context of web search, content is not expected to be available for search immediately.
Web crawlers periodically crawl web content and build indexes, using batch operations to achieve high throughput.
There, indexes are eventually consistent with the source data, and, depending on the type of the content,
indexing delays of minutes, hours of even days might be acceptable.
In the case of secondary indexing in non-relational distributed data stores,
there exists a spectrum of design choices for index maintenance.

\bigskip
\noindent

Azure DocumentDB is Microsoft’s multi-tenant distributed database service for managing JSON documents at large scale.
The work in \cite{shukla:schemaagnostic} describes DocumentDB's indexing subsystem.
DocumentDB supports two index update policies.
In ``consistent'' mode, the index is updated synchronously as part of the document update, and therefore
queries have the same consistency level as the level specified for the point-reads
(the available consistency levels are strong, bounded-staleness, session and eventual).
In ``lazy'' mode, the index is updated asynchronously when enough resources are available so that the indexing can proceed
without affecting the performance guarantees offered for user requests.
As a result, queries are eventually consistent.
This mode is better suited for ``ingest now, query later'' workloads requiring efficient document ingestion.
The indexing policy is set at the level of a document collection.

\bigskip
\noindent

Diff-Index \cite{tan:diffindex} and Hindex \cite{tang:deferredindexing} address the challenges of supporting secondary
indexes in data stores that use the log-structured merge tree as their primary storage data structure.
The characteristics of log-structured data stores (no in-place update, asymmetric read/write latency)
make the task of maintaining a fully consistent index with reasonable update performance particularly challenging.

Diff-Index \cite{tan:diffindex} proposes algorithms for multiple different levels of consistency between indexes and base data:
strong, eventual and sessions consistency.
The consistency levels make different trade-offs between index update latency and index consistency.
The consistency level can be chosen per index depending on the workload and consistency requirements.
We note that the authors use the term causal consistency for the consistency level in which once a write operation is
acknowledged to the client both data and associated index entries are persisted in the data store as;
in this work, we use the term strong consistency for this guarantee.
To implement eventual consistency, Diff-index maintains writes that require index processing in an in-memory queue.
Writes are immediately acknowledged to the client, and the index is updated by a background process.
Session consistency is implemented by tracking additional state in the client library.

Hindex \cite{tang:deferredindexing} decomposes the task of index maintenance into two sub-tasks:
inserting new index entries and removing old index entries (termed index-repair).
It executes the inexpensive insert task synchronously while deferring the expensive repair task.
This work proposes two scheduling strategies for the repair operations:
an offline repair that is coupled with the key-value store’s compaction mechanism,
and an online repair where index-repair operations are piggybacked in the execution path of query operations.

\bigskip
\noindent

Enabling configurable index (and, more generally, state) maintenance schemes is one of the design goals of this work.
This is achieved by: 1) supporting both synchronous (using acknowledgements) and asynchronous streams between QPUs,
and 2) allowing different QPU types do treat updates using different schemes.


\section{Materialized views}
\label{sec:mv}
Materialized views were devised for storing results of expensive query computations in relational databases.
Support for materialized view is limited \cite{mysql:mvs} and views must usually be rebuilt on change \cite{postgresql:mvs}.

The concept of materialized views has been employed by a number of application-level caching systems
\cite{kate:pequod, amiri:dbproxy, gjengset:noria}.

Noria \cite{gjengset:noria} is a web application backend,
designed with the aim of delivering the same fast reads as an in-memory cache in front of a database,
without the application having to manage the cache.
Noria's system design shares several similarities with our design.
Applications register long-term queries with Noria for repeated use.
Noria constructs a data-flow graph that connects database tables at its inputs to materialized views at its leaves,
and incrementally evaluates these queries when the underlying data changes.
Similarly to a QPU-based architecture, Noria’s data-flow is a directed acyclic graph of relational operators.
This is because both systems' designs are based on database query planning.
Noria generates the data-flow graph for an SQL query using a process similar to database query planning.

Pequod \cite{kate:pequod} and DBProxy \cite{amiri:dbproxy} are application-level caches that support incrementally
maintained, partially-materialized views \cite{zhou:dynamicmv}.
Both systems maintain materialized views by subscribing to a stream of updates propagated the underlying database,
using notifications mechanisms provided by the database.
Our design uses the same technique in order to propagate update information from the data store to the QPU graph.

Pequod, similarly to our design, acts as a write-around cache;
Application writes go directly to the database, and applications access Pequod only for reads.
Conversely, in Noria and DBProxy writes are submitted to the cache and transparently forwarded to the database.

In Noria and Pequod, view materialization is controlled by users.
Noria receives a program specifying a relational schema and a set of parametrized queries, written in SQL,
and installs a corresponding data-flow graph.
In addition, Noria is able to adapt its data-flow graph to new queries without downtime.
Similarly, in Pequod users can textually define views using a simple grammar.
On the other hand, DBProxy decides dynamically which views to caching and which can be evicted to save space.
It achieves that by transparently intercepting application SQL calls.
In this work, we focus on the system design aspect of maintaining materialized view over geo-distributed data.
We consider the aspects view selection and view definition to be out of the scope of this work.
In our current design, a QPU-graph (akin to Noria's data-flow graph) is defined by the system's operator by passing
the corresponding configuration to the graph's query processing units.
However, the task of configuration and deployment of a QPU graph based on a given query could be automatically performed
by an external component.

\section{Result caching}
\label{sec:related_caching}
In \cite{cambazoglu:yahoorefreshing} Cambazoglu et al. present the design of the result cache used
in the Yahoo! search engine.
The authors argue that eviction policies are not critical in the setting of search engines, as those engines have access
to large caches by using disk space to store query results.
Instead, this work focuses on the problem of keeping cached results consistent with the search engine’s index
as new batches of crawled documents are indexed.
It proposes a TTL-based strategy for expiring cache entries and refreshing them by issuing refresh queries,
and presents an algorithm for prioritizing cache entries to be refreshed based on the access frequency of entries and
the age of the cached entry.

In general, we can categorize cache invalidation and approaches in two types.
Coupled or ``push-based'' approaches provide the cache with information about changes to the underlying data
(the web search index in this case).
Decoupled approaches invalidate cached entries without any concrete knowledge of changes to the underlying data.
The work in \cite{cambazoglu:yahoorefreshing} presents a decoupled,
``pull-based'' approach in which the caching system refreshes cache entries by querying the underlying index.
Our design supports both push- and pull-based cache invalidation and refreshing:
Pull-based refreshing is implemented by having a cache QPU send query requests to QPUs that implement the underlying
query engine.
Push-based refreshing is implemented by having a cache QPU subscribe to notifications for a cache entry on a cache
miss.
The invalidation and refresh policy of a cache QPU is controlled by a configuration parameter,
and can be adjusted by operators according to the characteristics of each use case.
In addition, the two approaches can be also combined in a hybrid approach in which a cache QPU receives simple notifications
for updates to a cache entry, without receiving information about the content of those updates,
and refreshes the cache entry using a query request when a threshold expressed in number of updates is crossed.

\section{Geo-distributed query processing}
\label{sec:federation}

A series of papers \cite{cambazoglu:multisitequantifying, yates:multisitefeasibility, cambazoglu:multisiteforwarding, frances:multisiteefficiency, kayaaslan:multisitereplication}
have studied the problem of designing a large-scale web search engine over multiple geographically distributed data centers.

These works consider the following system model:
A search engine architecture composed of multiple data centers, each associated with a geographical region.
Each data center stores and crawls documents that are served by web sites in its geographical region.
As a result, each data center is responsible for a disjoint subset of the Web.
The search engine builds an inverted index over the crawled documents, which is used to serve queries.
Documents are ranked based on a ranking function; the result of a query consists of the $k$ most highly ranked documents
for that query.

\bigskip
\noindent
In \cite{cambazoglu:multisitequantifying}, Cambazoglu et al. present a taxonomy of search engine architectures for this problem.
In a \textit{centralized} architecture, the entire index is stored in one, central site, and all user queries are submitted to this site.
The index might be replicated or partitioned within the site.
This architecture was used by early web search engines as well as small-scale engines.

In a \textit{replicated} architecture, the entire index is replicated over multiple data centers.
Queries are routed to data centers based on geographical proximity of users to data centers.

Finally, in a \textit{partitioned} architecture, the document collection is partitioned into smaller, non-overlapping
sub-collections such that each data center is responsible for a different sub-collection.
Queries originating in a particular region are evaluated over the partial index in the corresponding search site.
The underlying assumption in this approach is that users are interested more in documents located in their own region,
and local documents are more relevant for queries originating from the same region.

This problem is similar to the problem of multi-cloud federated metadata search discussed in Section~\ref{sec:zenko}.
Similarly to the multi-site web search problem,
in multi-cloud metadata search a corpus is partitioned into disjoint datasets, each placed on a geographically distant
data center.
Thanks to its flexibility, our architecture design supports all three types of search engine architectures discussed in
\cite{cambazoglu:multisitequantifying}.
This use case is amongst the main motivations for this work.

\bigskip
\noindent
There are however two main differences between the problem examined in this work and multi-site web search.
First, our query model does not involve the notion of a ranking function.
In both problems, the challenge with the partitioned architecture is to retrieve relevant query results that are not
local to the site serving a given query.
Taking advantage of the ranked query results, approaches to multi-site web search have proposed two solutions:
partial replication of the popular documents across search sites \cite{frances:multisiteefficiency},
and selectively forwarding queries to remote sites that are expected to contribute relevant query results
\cite{yates:multisitefeasibility, cambazoglu:multisiteforwarding}.
In this work, we rely on techniques such as caching and replication of popular index and materialized view entries in
order to reduce the need to forward queries to remote sites.
Cambazoglu et al. \cite{cambazoglu:multisiteforwarding} have evaluated the impact of result caching
on the rate of locally processed queries.
Results show that with result caching, the fraction of queries that can be locally processed inc increases by 35\% to 45\%.

Second, the problem of web search has a different index update model than the problem of attribute-based querying.
In web search, indexes are updated periodically based on information from crawlers,
while attribute-based querying involves incremental index updates,
and potentially high rates of updates.

% The downside of this approach is that documents that are relevant to a query but are not local to the search site are
% not retrieved, leading to inferior search quality.
% The problem of accessing non-local documents has two immediate solutions:
% taking the data to where it is sought and/or taking the queries to what they seek.
% The first is an offline solution that requires partial replication of the popular documents in a region on some non-local search sites.
% The second is an online solution that requires selective forwarding of queries between search sites to extend cover- age of search results.
% forwarding of queries between search sites to extend cover- age of search results.

% The works in \cite{yates:multisitefeasibility} and \cite{cambazoglu:multisiteforwarding}
% have proposed techniques for selective query forwarding.
% Selective forwarding works as follows:
% When a search site receives a query, it estimates the quality of the locally computed results relative to globally computed results
% (results that which would have been obtained through evaluation over the full index).
% If it is determined that the local index misses some documents that would have appeared in the global ranking,
% the local site estimates which remote sites might have those documents, and forwards the query to those sites.
% Finally, non-local and local results are merged and returned to the user.

% In \cite{frances:multisiteefficiency} Kayaaslan et al.
% propose strategies for selectively replicating documents across search engine sites.
% The key idea is to identify documents that are of interest to the users of certain geographical regions,
% based on the occurrence frequencies of documents in past search results,
% and then replicate identified documents on remote sites so that future queries can be processed without the need for
% forwarding.
% Documents replication leads to improvements in the query processing efficiency, as when performed effectively,
% fewer queries need to be forwarded, this reducing average query response times and the overall query workload of the search engine.
% At the same time, increasing the volume of replicated documents has a negative impact on query processing times of local data centers,
% due to the increase in their index sizes.
% There is thus a trade-off between replication and search performance.
% The work in \cite{kayaaslan:multisitereplication}
% proposes three different document replication strategies, each optimizing a different objective:
% reducing the potential search quality loss, the average query response time, or the total query workload of the search system.



\section{Modular \& Composable architectures}
\label{sec:modular_arch}
Click \cite{kohler:click} proposes a software architecture for building flexible and configurable routers.
A Click router is assembled from packet processing modules, called elements.
A Click element represents a unit of router processing.
Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices.
A router configuration is a directed graph with elements at its vertices.
An edge between two elements represents a possible path for packet; packets flow along the edges of the graph.

While aimed at a different field, Click's core philosophy is close the QPU model.
A Click element, akin to a query processing unit, is responsible for a simple packet processing function,
and different types of elements are responsible for different packet processing tasks.

Pleiades \cite{bouget:pleiades} is a framework for constructing and enforcing large-scale distributed structural invariants.
It addressed the need of system architectures to organize nodes in complex topologies.
Pleiades is based on assembly-based modularity for enabling configurations to express complex topologies:
A structural invariant is expressed as a combination of basic shapes, such as rings, grids, and stars.
A configurations specifies a set of basic shapes, and how these shapes should be connected.

\section{State and computation placement}
\label{sec:placement}
\subsection{Computation placement}

The problem of the placement of computations on processing nodes has been studied primarily in the context of distributed
stream processing systems \cite{lakshmanan:placementstrategies}.

A stream processing system can be viewed as a flow graph composed of a set of message sources (producers),
and a collection of stream processing operators that periodically consume messages, perform processing tasks,
and deliver results either to message sinks (consumers) or to other operators.
In addition, there exists a collection of processing nodes available for deploying operators.
The operator placement problem consists of computing an assignment of operators to nodes that optimally satisfies a
certain metric, such as load, latency, or network resource usage.

From an architecture point of view, placement decisions are performed either by a central placement module, independent
from the stream processing engine, or are decentralized, each operator taking local placement decisions.
Our work currently consider the placement decisions as independent to the query processing tasks;
the placement of the QPU graph on system nodes is decided on by an central entity (a system operator in our current design)
before the system is deployed.
A direction for future work could be a hybrid placement logic, in which after an initial global placement decision,
individual QPUs make local decisions in runtime in response to changing workload, network and resource conditions.

Placement algorithms can be categorized to centralized and decentralized.
Centralized placement algorithms require global view of the system, including workload information and resource availability.
These algorithms can compute globally optimal placement assignments, but often have limited scalability.
On the other had, decentralized algorithms achieve improved scalability by making placement decisions based on local
workload and resource information.
Our design implicitly assumes that an system operator deciding on the placement of a QPU graph has a global view of the
system.

An important difference with these works is that in our current design placement decisions are performed offline,
before a QPU graph is deployed, and placement is static throughput the system's operation.

Another distinction is that, most often, this approaches consider stateless operators.
In this work we propose an approach for enabling flexible placement of not only stateless operators, such as
filters, but stateful components such and indexes and materialized views.
In addition, we take into account techniques such as partitioning and replication of query processing state.
We consider this one of our mains contributions.

More generally, in this work, we focus on the \textit{mechanisms} (as opposed to placement algorithms)
required for enabling flexible placement.
While we evaluate different placement configurations in order to demonstrate the benefits of flexile placement,
we consider placement algorithms an orthogonal problem that is out of the scope of this work.

\bigskip
\noindent
Two optimization techniques often employed by operator placement algorithms are operator reuse and replication.
Operator reuse is based on the observation that consumers commonly execute identical of partially identical queries.
Instead of instantiating new operators for each query,
some placement algorithms \cite{pietzuch:networkawareopplacement} try to detect opportunities for sharding intermediate results across queries.
This avoids transmitting multiple redundant copies of the same data.
The design and implementation of query processing units includes two mechanisms that enable operator reuse.
First, each QPU can perform multiple independent processing tasks in parallel.
As a result, queries that require the functionality of a certain QPU can share the same QPU instance,
provided there are enough resources.
Second, when a QPU receives a query request, it first examines if that particular query is already being executed,
and if so it appends the component that submitted the query to the receivers of the query's output stream.

Operator replication consists of deploying multiple instances of an operator,
and partitioning the input stream among operator replicas.
This enhances the scalability of stream processing as it parallelizes the processing task among operator replicas
\cite{stonebraker:streamprocessingrequirements}.
This technique has been also applied to distributed query processing.
Query processing in CockroachDB \cite{cockroachdb:distsql} uses the concept of grouping to partition a logical stream
(that is part of a query processing task) into multiple physical streams which can be processed in parallel.
A stream is divided into groups so that computation within a group is independent from other groups.
Parallelizing query processing across operator replicas is also possible in QPU-based architectures.
The QPU graph and configuration for achieving that is akin to the index partitioning scheme presented in Section~\ref{sec:cs_index_partitioning}.

We note that in the the scope of this work neither operator reuse nor replication are automatically handled by the system,
as in some stream processing systems.
The system however provides the functionalities required for putting in place these techniques.
Employing operator replication and reuse in a QPU graph can be performed by configuring the graph accordingly.

\bigskip
\noindent
Helios \cite{potharaju:helios} is a system used at Microsoft for ingestion, indexing, and aggregation of large streams
of real-time data.
Helios is used to collect very large amounts (in the order of several PB per day) across 15 data centers.
It was developed in response to limitation of existing systems
(``None of our existing systems could handle these requirements adequately; either the solution did not scale or it was too expensive to capture all the desired telemetry.'')
Amongst Helios' principal design decisions is to support computation offloading at the edge.
In Helios' architecture, data collection is performed by an independently-running executable that can run in any pod/rack/data center.
Using this components the tasks of data summarization, index generation, and aggregation can be distributed to edge devices.

Helios' support for computation offloading at the edge is similar to the core design goal of the architecture proposed in the work:
enabling flexible placement of both (query processing) state and computation across the system.
While we have focused on scenarios in which data sources reside in the data center,
QPU-based architectures are not restricted to such configurations and can be also used to ingest and index data from geo-distributed
sources.
Helios is, of course, a system deployed in production in Microsoft and provides support for aspects that are out of the
scope of this work, such as the ability to impose restrictions on the resource consumption of the data collection component
so that in can run on resource constrained devices.

\subsection{State placement}
Large scale web services are concurrently used by a large number of geographically distributed clients.
A significant challenge that these services face is the large request latencies resulting from the geographical distance
between clients and application state.
A common solution to this latency problem is to place data closer to clients in order to avoid costly round-trips
to the data center.
For example, Google has comparatively few data center locations relative to edge nodes \cite{google:infra}.

Content delivery networks (CDNs), such as Akamai \footnote{https://www.akamai.com} aim at deploying data on edge nodes,
close to clients.
However, CDNs focus on static data as such images and video content and are not well suited for mutable state.

Other works have studied the problem placing application state across data centers.
Volley \cite{agarwal:volley} is a system for automatic data placement across geo-distributed data centers.
Volley makes placement decisions based on request logs submitted by applications.
The difference between the placement of application state and derived state used for query processing (indexes, materialized)
views, is that efficient placement decisions about application state result in reduced latency for both reads and writes
to the state.
However, in the system model considered in this work, base data has a static placement,
while derive state can be flexibly placed across the system.
This often results to trade-offs between read and write operations,
as derived state receives read requests from the clients, and update notifications from the base data.

% update propagation across different edges and processing at different vertices can happen in parallel.
% Therefore, data-flow processing is well-suited to scaling across multiple CPU cores and servers.


% \section{Scaling search services through partitioning and replication}
% It is widely known that caching, partitioning, and replication are three primary strategies for scaling up large,
% distributed web services (e.g., search) with high-throughput and low-latency requirements.
% sharing the distributed search architecture that underlies Twitter user search, a service for discovering relevant accounts on the popular micro-blogging service.
% issues that are critical to their operation in real-world production environments.
% Focusing in particular on partitioning and replication, some of these important questions include:
% manage partition and replica configurations in coordinate automatic, seamless failover of partition and replica
% dynamically adjust configurations in response to increasing (or decreasing) load in an elastic manner
% Our goal is to share experiences in answering the above questions, in the context of user search
% The solution that we describe makes extensive use of ZooKeeper
% make use of the design principle that eliminates the distinction between failures and other anticipated service disruptions (e.g., code deployments, configuration changes, etc.)
% explication of many production and operational issues associated with partitioned, replicated search services.

\section{Distributed computation models}
\label{sec:dataflowmapreduce}

\subsection{MapReduce}

MapReduce \cite{dean:mapreduce} is a programming paradigm for distributing computational tasks over multiple processing nodes.
The core idea is that many types of tasks can be expressed as a map operation that operates on the dataset which is organized
as a collection of records (key-value pairs);
The map operation can be distributed, each individual operation processing a different subset of the input dataset.
The output of these map operations can be then collected and merged into the desired result by reduce operations.

McCreadie et al. \cite{mccreadie:mapreduceindexing} have examined the benefits that can be obtained by performing document
indexing using MapReduce.
In particular, this work presents and evaluates four strategies for applying the MapReduce paradigm
to the task of indexing large document collections.

The query processing unit computation model could be compared with the MapReduce paradigm.
Two central characteristics of MapReduce are that:
1) each map task is independent and not aware of its context in the overall job
(because of this that map tasks can be performed by a large number of workers in parallel, each operating on a different
subset of the input dataset)
and, 2) map and reduce tasks form a two-tiered hierarchical structure,
with the output of the map tasks being used as input to the reduce tasks.
The QPU model has similar characteristics:
Each individual QPU is independent, and communicates with its connection in the graph graph through well-defined interfaces;
Connections between QPUs from from tiered architectures in which the output of one tier is used as input to the next.
Because of these similarities, a QPU graph can be configured to perform MapReduce-style distributed computations.
For example, the task of indexing a large document collection, can be implemented with a QPU-based architecture
as follows:
``Indexing'' QPUs, responsible for receiving documents as input, can be used as map tasks.
For each document, an indexing QPU computed and emits a (term, docID) records.
``Reduce'' QPUs be connected a parents of multiple indexing QPUs,
so that each reduce QPU receives the output of multiple indexing QPUs.
Reduce QPU then merge (term, docID) records into posting lists for each docID.

On the other hand, the MapReduce paradigm and the QPU model have important differences in their design goals.
MapReduced has been developed with the aim of parallelizing and distributing one-time computational tasks,
while the QPU model is mainly intended for building and incrementally maintaining derived state.

\subsection{Dataflow engines}
An evolution of the MapReduce model is the model used in dataflow execution engines.
Dataflow engines explicitly model the flow of data through processing stages (henge the term dataflow).
A dataflow system represents computations as a graph, in which vertices are user-defined operators.
An operator receives input records, process each record and emits zero or more output records.
Edge carry records between operators; the output of one operator becomes the input of another.
Some of the most well known systems that perform dataflow computations at their core is
Apache Spark \cite{zaharia:spark} and Flink \cite{carbone:flink}.

These systems are based on research systems such as Dryad.
Dryad \cite{isard:dryad} is a general-purpose distributed execution engine that employs the dataflow model.
A Dryad job is a directed acyclic graph; each vertex is a program and edges represent data channels.
Dryad's runtime is responsible for mapping a logical computation graph to physical resources,
and abstracts several distributed computing problems from the developer,
such as resource allocation, scheduling, and the transient or permanent failures.

Noria \cite{gjengset:noria} proposes partially-stateful dataflow,
a dataflow model that supports eviction and reconstruction of data-flow state on demand.
Noria employs partially-stateful dataflow to support partially materialized view maintained
using updates from a database system.

The QPU computation model also makes use of dataflow computations for maintaining derived state
over base data.
An important difference from general dataflow engines is that the QPU model is designed for executing
query processing computations rather than arbitrary distributed computations.
