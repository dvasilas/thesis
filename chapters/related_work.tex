\section{Federated query processing}
\label{sec:federation}
\subsection{Multi-site web search}
A series of papers \cite{cambazoglu:multisitequantifying, yates:multisitefeasibility, cambazoglu:multisiteforwarding, frances:multisiteefficiency, kayaaslan:multisitereplication}
have studied the problem of designing a large-scale web search engine over multiple geographically distributed data centers.

These works consider the following system model:
A search engine architecture composed of multiple data centers, each associated with a geographical region.
Each data center stores and crawls documents that are served by web sites in its geographical region.
As a result, each data center is responsible for a disjoint subset of the Web.
The search engine builds an inverted index over the crawled documents, which is used to serve queries.
Documents are ranked based on a ranking function; the result of a query consists of the $k$ most highly ranked documents
for that query.

\bigskip
\noindent
In \cite{cambazoglu:multisitequantifying}, Cambazoglu et al. present a taxonomy of search engine architectures for this problem.
In a \textit{centralized} architecture, the entire index is stored in one, central site, and all user queries are submitted to this site.
The index might be replicated or partitioned within the site.
This architecture was used by early web search engines as well as small-scale engines.

In a \textit{replicated} architecture, the entire index is replicated over multiple data centers.
Queries are routed to data centers based on geographical proximity of users to data centers.

Finally, in a \textit{partitioned} architecture, the document collection is partitioned into smaller, non-overlapping
sub-collections such that each data center is responsible for a different sub-collection.
Queries originating in a particular region are evaluated over the partial index in the corresponding search site.
The underlying assumption in this approach is that users are interested more in documents located in their own region,
and local documents are more relevant for queries originating from the same region.

This problem bares several similarities to the problem of multi-cloud federated metadata search discussed in Section~\ref{sec:zenko}.
Similarly to the multi-site web search problem,
in multi-cloud metadata search a corpus is partitioned into disjoint datasets, each placed on a geographically distant
data center.
Thanks to its flexibility, our architecture design supports all three types of search engine architectures discussed in
\cite{cambazoglu:multisitequantifying}.
This use case is amongst the main motivations for this work.

\bigskip
\noindent
There are however two main differences between the problem examined in this work and multi-site web search.
First, our query model does not involve the notion of a ranking function.
In both problems, the challenge with the partitioned architecture is to retrieve relevant query results that are not
local to the site serving a given query.
Taking advantage of the ranked query results, approaches to multi-site web search have proposed two solutions:
partial replication of the popular documents across search sites \cite{frances:multisiteefficiency},
and selectively forwarding queries to remote sites that are expected to contribute relevant query results
\cite{yates:multisitefeasibility, cambazoglu:multisiteforwarding}.
In this work, we rely on techniques such as caching and replication of popular index and materialized view entries in
order to reduce the need to forward queries to remote sites.
Cambazoglu et al. \cite{cambazoglu:multisiteforwarding} have evaluated the impact of result caching
on the rate of locally processed queries.
Results show that with result caching, the fraction of queries that can be locally processed inc increases by 35\% to 45\%.

Second, the problem of web search has a different index update model than the problem of attribute-based querying.
In web search, indexes are updated periodically based on information from crawlers,
while attribute-based querying involves incremental index updates,
and potentially high rates of updates.

% The downside of this approach is that documents that are relevant to a query but are not local to the search site are
% not retrieved, leading to inferior search quality.
% The problem of accessing non-local documents has two immediate solutions:
% taking the data to where it is sought and/or taking the queries to what they seek.
% The first is an offline solution that requires partial replication of the popular documents in a region on some non-local search sites.
% The second is an online solution that requires selective forwarding of queries between search sites to extend cover- age of search results.
% forwarding of queries between search sites to extend cover- age of search results.

% The works in \cite{yates:multisitefeasibility} and \cite{cambazoglu:multisiteforwarding}
% have proposed techniques for selective query forwarding.
% Selective forwarding works as follows:
% When a search site receives a query, it estimates the quality of the locally computed results relative to globally computed results
% (results that which would have been obtained through evaluation over the full index).
% If it is determined that the local index misses some documents that would have appeared in the global ranking,
% the local site estimates which remote sites might have those documents, and forwards the query to those sites.
% Finally, non-local and local results are merged and returned to the user.

% In \cite{frances:multisiteefficiency} Kayaaslan et al.
% propose strategies for selectively replicating documents across search engine sites.
% The key idea is to identify documents that are of interest to the users of certain geographical regions,
% based on the occurrence frequencies of documents in past search results,
% and then replicate identified documents on remote sites so that future queries can be processed without the need for
% forwarding.
% Documents replication leads to improvements in the query processing efficiency, as when performed effectively,
% fewer queries need to be forwarded, this reducing average query response times and the overall query workload of the search engine.
% At the same time, increasing the volume of replicated documents has a negative impact on query processing times of local data centers,
% due to the increase in their index sizes.
% There is thus a trade-off between replication and search performance.
% The work in \cite{kayaaslan:multisitereplication}
% proposes three different document replication strategies, each optimizing a different objective:
% reducing the potential search quality loss, the average query response time, or the total query workload of the search system.

\section{Result caching}
\label{sec:caching}
In \cite{cambazoglu:yahoorefreshing} Cambazoglu et al. present the design of the result cache used
in the Yahoo! search engine.
The authors argue that eviction policies are not critical in the setting of search engines, as those engines have access
to large caches by using disk space to store query results.
Instead, this work focuses on the problem of keeping cached results consistent with the search engineâ€™s index
as new batches of crawled documents are indexed.
It proposes a TTL-based strategy for expiring cache entries and refreshing them by issuing refresh queries,
and presents an algorithm for prioritizing cache entries to be refreshed based on the access frequency of entries and
the age of the cached entry.

In general, we can categorize cache invalidation and approaches in two types.
Coupled or ``push-based'' approaches provide the cache with information about changes to the underlying data
(the web search index in this case).
Decoupled approaches invalidate cached entries without any concrete knowledge of changes to the underlying data.
The work in \cite{cambazoglu:yahoorefreshing} presents a decoupled,
``pull-based'' approach in which the caching system refreshes cache entries by querying the underlying index.
Our design supports both push- and pull-based cache invalidation and refreshing:
Pull-based refreshing is implemented by having a cache QPU send query requests to QPUs that implement the underlying
query engine.
Push-based refreshing is implemented by having a cache QPU subscribe to notifications for a cache entry on a cache
miss.
The invalidation and refresh policy of a cache QPU is controlled by a configuration parameter,
and can be adjusted by operators according to the characteristics of each use case.
In addition, the two approaches can be also combined in a hybrid approach in which a cache QPU receives simple notifications
for updates to a cache entry, without receiving information about the content of those updates,
and refreshes the cache entry using a query request when a threshold expressed in number of updates is crossed.

\section{State and computation placement}
\label{sec:placement}
\subsection{Computation placement}

The problem of the placement of computations on processing nodes has been studied primarily in the context of distributed
stream processing systems \cite{lakshmanan:placementstrategies}.

A stream processing system can be viewed as a flow graph composed of a set of message sources (producers),
and a collection of stream processing operators that periodically consume messages, perform processing tasks,
and deliver results either to message sinks (consumers) or to other operators.
In addition, there exists a collection of processing nodes available for deploying operators.
The operator placement problem consists of computing an assignment of operators to nodes that optimally satisfies a
certain metric, such as load, latency, or network resource usage.

From an architecture point of view, placement decisions are performed either by a central placement module, independent
from the stream processing engine, or are decentralized, each operator taking local placement decisions.
Our work currently consider the placement decisions as independent to the query processing tasks;
the placement of the QPU graph on system nodes is decided on by an central entity (a system operator in our current design)
before the system is deployed.
A direction for future work could be a hybrid placement logic, in which after an initial global placement decision,
individual QPUs make local decisions in runtime in response to changing workload, network and resource conditions.

Placement algorithms can be categorized to centralized and decentralized.
Centralized placement algorithms require global view of the system, including workload information and resource availability.
These algorithms can compute globally optimal placement assignments, but often have limited scalability.
On the other had, decentralized algorithms achieve improved scalability by making placement decisions based on local
workload and resource information.
Our design implicitly assumes that an system operator deciding on the placement of a QPU graph has a global view of the
system.

An important difference with these works is that in our current design placement decisions are performed offline,
before a QPU graph is deployed, and placement is static throughput the system's operation.

Another distinction is that, most often, this approaches consider stateless operators.
In this work we propose an approach for enabling flexible placement of not only stateless operators, such as
filters, but stateful components such and indexes and materialized views.
In addition, we take into account techniques such as partitioning and replication of query processing state.
We consider this one of our mains contributions.

More generally, in this work, we focus on the \textit{mechanisms} (as opposed to placement algorithms)
required for enabling flexible placement.
While we evaluate different placement configurations in order to demonstrate the benefits of flexile placement,
we consider placement algorithms an orthogonal problem that is out of the scope of this work.

\bigskip
\noindent
Two optimization techniques often employed by operator placement algorithms are operator reuse and replication.
Operator reuse is based on the observation that consumers commonly execute identical of partially identical queries.
Instead of instantiating new operators for each query,
some placement algorithms \cite{pietzuch:networkawareopplacement} try to detect opportunities for sharding intermediate results across queries.
This avoids transmitting multiple redundant copies of the same data.
The design and implementation of query processing units includes two mechanisms that enable operator reuse.
First, each QPU can perform multiple independent processing tasks in parallel.
As a result, queries that require the functionality of a certain QPU can share the same QPU instance,
provided there are enough resources.
Second, when a QPU receives a query request, it first examines if that particular query is already being executed,
and if so it appends the component that submitted the query to the receivers of the query's output stream.

Operator replication consists of deploying multiple instances of an operator,
and partitioning the input stream among operator replicas.
This enhances the scalability of stream processing as it parallelizes the processing task among operator replicas
\cite{stonebraker:streamprocessingrequirements}.
This technique has been also applied to distributed query processing.
Query processing in CockroachDB \cite{cockroachdb:distsql} uses the concept of grouping to partition a logical stream
(that is part of a query processing task) into multiple physical streams which can be processed in parallel.
A stream is divided into groups so that computation within a group is independent from other groups.
Parallelizing query processing across operator replicas is also possible in QPU-based architectures.
The QPU graph and configuration for achieving that is akin to the index partitioning scheme presented in Section~\ref{sec:cs_index_partitioning}.

We note that in the the scope of this work neither operator reuse nor replication are automatically handled by the system,
as in some stream processing systems.
The system however provides the functionalities required for putting in place these techniques.
Employing operator replication and reuse in a QPU graph can be performed by configuring the graph accordingly.

\subsection{State placement}
Large scale web services are concurrently used by a large number of geographically distributed clients.
A significant challenge that these services face is the large request latencies resulting from the geographical distance
between clients and application state.
A common solution to this latency problem is to place data closer to clients in order to avoid costly round-trips
to the data center.
For example, Google has comparatively few data center locations relative to edge nodes \cite{google:infra}.

Content delivery networks (CDNs), such as Akamai \footnote{https://www.akamai.com} aim at deploying data on edge nodes,
close to clients.
However, CDNs focus on static data as such images and video content and are not well suited for mutable state.

Other works have studied the problem placing application state across data centers.
Volley \cite{agarwal:volley} is a system for automatic data placement across geo-distributed data centers.
Volley makes placement decisions based on request logs submitted by applications.
The difference between the placement of application state and derived state used for query processing (indexes, materialized)
views, is that efficient placement decisions about application state result in reduced latency for both reads and writes
to the state.
However, in the system model considered in this work, base data has a static placement,
while derive state can be flexibly placed across the system.
This often results to trade-offs between read and write operations,
as derived state receives read requests from the clients, and update notifications from the base data.

\section{MapReduce}
\label{sec:mapreduce}
MapReduce \cite{dean:mapreduce} is a programming paradigm for distributing computational tasks over multiple processing nodes.
The core idea is that many types of tasks can be expressed as a map operation that operates on the dataset which is organized
as a collection of records (key-value pairs);
The map operation can be distributed, each individual operation processing a different subset of the input dataset.
The output of these map operations can be then collected and merged into the desired result by reduce operations.

McCreadie et al. \cite{mccreadie:mapreduceindexing} have examined the benefits that can be obtained by performing document
indexing using MapReduce.
In particular, this work presents and evaluates four different strategies for applying the MapReduce paradigm
to the task of indexing very large document collections.

The query processing unit computation model bares certain similarities to the MapReduce paradigm.
In MapReduce, each map task is independent and not aware of its context in the overall job.
Because of this property, the map task can be performed by a large number of map workers in parallel,
each operating on a different subset of the input dataset.
Additionally, map and reduce tasks form a two-tiered hierarchical structure,
with the output of the map task being used as input to the reduce task.
Similarly, each individual QPU is independent and not aware of the overall graph.
It operates in response to receiving query requests by one or more ``parent'' QPUs,
and optionally sending itself query requests to ``child'' QPUs.
This parent-child relationship can be used to express the connection between map and reduce tasks.
In addition, input to both the map and reduce tasks can be expressed as QPU input streams.
As a result, while not originally designed with that goal in mind,
the QPU-based architecture model can be used to express MapReduce-style distributed computations.
For example, the task of indexing a large document collection, can be implemented with a QPU-based architecture
as follows:
``Indexing'' QPUs, responsible for receiving documents as input, can be used as map tasks.
For each document, an indexing QPU computed and emits a (term, docID) records.
``Reduce'' QPUs be connected a parents of multiple indexing QPUs,
so that each reduce QPU receives the output of multiple indexing QPUs.
Reduce QPU then merge (term, docID) records into posting lists for each docID.