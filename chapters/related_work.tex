\section{State and computation placement}
\label{sec:placement}
\subsection{Computation placement}

The problem of the placement of computations on processing nodes has been studied primarily in the context of distributed
stream processing systems \cite{lakshmanan:placementstrategies}.

A stream processing system can be viewed as a flow graph composed of a set of message sources (producers),
and a collection of stream processing operators that periodically consume messages, perform processing tasks,
and deliver results either to message sinks (consumers) or to other operators.
In addition, there exists a collection of processing nodes available for deploying operators.
The operator placement problem consists of computing an assignment of operators to nodes that optimally satisfies a
certain metric, such as load, latency, or network resource usage.

From an architecture point of view, placement decisions are performed either by a central placement module, independent
from the stream processing engine, or are decentralized, each operator taking local placement decisions.
Our work currently consider the placement decisions as independent to the query processing tasks;
the placement of the QPU graph on system nodes is decided on by an central entity (a system operator in our current design)
before the system is deployed.
A direction for future work could be a hybrid placement logic, in which after an initial global placement decision,
individual QPUs make local decisions in runtime in response to changing workload, network and resource conditions.

Placement algorithms can be categorized to centralized and decentralized.
Centralized placement algorithms require global view of the system, including workload information and resource availability.
These algorithms can compute globally optimal placement assignments, but often have limited scalability.
On the other had, decentralized algorithms achieve improved scalability by making placement decisions based on local
workload and resource information.
Our design implicitly assumes that an system operator deciding on the placement of a QPU graph has a global view of the
system.

An important difference with these works is that in our current design placement decisions are performed offline,
before a QPU graph is deployed, and placement is static throughput the system's operation.

Another distinction is that, most often, this approaches consider stateless operators.
In this work we propose an approach for enabling flexible placement of not only stateless operators, such as
filters, but stateful components such and indexes and materialized views.
In addition, we take into account techniques such as partitioning and replication of query processing state.
We consider this one of our mains contributions.

More generally, in this work, we focus on the \textit{mechanisms} (as opposed to placement algorithms)
required for enabling flexible placement.
While we evaluate different placement configurations in order to demonstrate the benefits of flexile placement,
we consider placement algorithms an orthogonal problem that is out of the scope of this work.

\medskip
\noindent
Two optimization techniques often employed by operator placement algorithms are operator reuse and replication.
Operator reuse is based on the observation that consumers commonly execute identical of partially identical queries.
Instead of instantiating new operators for each query,
some placement algorithms \cite{pietzuch:networkawareopplacement} try to detect opportunities for sharding intermediate results across queries.
This avoids transmitting multiple redundant copies of the same data.
The design and implementation of query processing units includes two mechanisms that enable operator reuse.
First, each QPU can perform multiple independent processing tasks in parallel.
As a result, queries that require the functionality of a certain QPU can share the same QPU instance,
provided there are enough resources.
Second, when a QPU receives a query request, it first examines if that particular query is already being executed,
and if so it appends the component that submitted the query to the receivers of the query's output stream.

Operator replication consists of deploying multiple instances of an operator,
and partitioning the input stream among operator replicas.
This enhances the scalability of stream processing as it parallelizes the processing task among operator replicas
\cite{stonebraker:streamprocessingrequirements}.
This technique has been also applied to distributed query processing.
Query processing in CockroachDB \cite{cockroachdb:distsql} uses the concept of grouping to partition a logical stream
(that is part of a query processing task) into multiple physical streams which can be processed in parallel.
A stream is divided into groups so that computation within a group is independent from other groups.
Parallelizing query processing across operator replicas is also possible in QPU-based architectures.
The QPU graph and configuration for achieving that is akin to the index partitioning scheme presented in Section~\ref{sec:cs_index_partitioning}.

We note that in the the scope of this work neither operator reuse nor replication are automatically handled by the system,
as in some stream processing systems.
The system however provides the functionalities required for putting in place these techniques.
Employing operator replication and reuse in a QPU graph can be performed by configuring the graph accordingly.

\subsection{State placement}
Large scale web services are concurrently used by a large number of geographically distributed clients.
A significant challenge that these services face is the large request latencies resulting from the geographical distance
between clients and application state.
A common solution to this latency problem is to place data closer to clients in order to avoid costly round-trips
to the data center.
For example, Google has comparatively few data center locations relative to edge nodes \cite{google:infra}.

Content delivery networks (CDNs), such as Akamai \footnote{https://www.akamai.com} aim at deploying data on edge nodes,
close to clients.
However, CDNs focus on static data as such images and video content and are not well suited for mutable state.

Other works have studied the problem placing application state across data centers.
Volley \cite{agarwal:volley} is a system for automatic data placement across geo-distributed data centers.
Volley makes placement decisions based on request logs submitted by applications.
The difference between the placement of application state and derived state used for query processing (indexes, materialized)
views, is that efficient placement decisions about application state result in reduced latency for both reads and writes
to the state.
However, in the system model considered in this work, base data has a static placement,
while derive state can be flexibly placed across the system.
This often results to trade-offs between read and write operations,
as derived state receives read requests from the clients, and update notifications from the base data.

\section{MapReduce}
MapReduce \cite{dean:mapreduce} is a programming paradigm for distributing computational tasks over multiple processing nodes.
The core idea is that many types of tasks can be expressed as a map operation that operates on the dataset which is organized
as a collection of records (key-value pairs);
The map operation can be distributed, each individual operation processing a different subset of the input dataset.
The output of these map operations can be then collected and merged into the desired result by reduce operations.

McCreadie et al. \cite{mccreadie:mapreduceindexing} have examined the benefits that can be obtained by performing document
indexing using MapReduce.
In particular, this work presents and evaluates four different strategies for applying the MapReduce paradigm
to the task of indexing very large document collections.

The query processing unit computation model bares certain similarities to the MapReduce paradigm.
In MapReduce, each map task is independent and not aware of its context in the overall job.
Because of this property, the map task can be performed by a large number of map workers in parallel,
each operating on a different subset of the input dataset.
Additionally, map and reduce tasks form a two-tiered hierarchical structure,
with the output of the map task being used as input to the reduce task.
Similarly, each individual QPU is independent and not aware of the overall graph.
It operates in response to receiving query requests by one or more ``parent'' QPUs,
and optionally sending itself query requests to ``child'' QPUs.
This parent-child relationship can be used to express the connection between map and reduce tasks.
In addition, input to both the map and reduce tasks can be expressed as QPU input streams.
As a result, while not originally designed with that goal in mind,
the QPU-based architecture model can be used to express MapReduce-style distributed computations.
For example, the task of indexing a large document collection, can be implemented with a QPU-based architecture
as follows:
``Indexing'' QPUs, responsible for receiving documents as input, can be used as map tasks.
For each document, an indexing QPU computed and emits a (term, docID) records.
``Reduce'' QPUs be connected a parents of multiple indexing QPUs,
so that each reduce QPU receives the output of multiple indexing QPUs.
Reduce QPU then merge (term, docID) records into posting lists for each docID.