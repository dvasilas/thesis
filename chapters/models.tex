\begin{tcolorbox}
\textbf{Outline Description - Internal}
The goal of this chapter is to prepare the ground so that the reader has the mental framework needed to understand the
rest of this document.

This includes:
\begin{itemize}
  \item Establishing the terminology to be used throughout the document.
  \item Establishing a ''world view'' by describing the system and data model that we consider, and the assumptions that
  we make
  \item Establishing concepts that will guide our design decisions in the following chapters (performance metrics etc.)
\end{itemize}
\end{tcolorbox}


\section{Models}
We consider a data serving system with a two-tiered architecture: a data storage tier and a query processing tier.

\begin{itemize}
  \item The data storage tier is responsible for storing and providing access to data.
  We use the terms \textit{storage system} or \textit{data store} for the system that implement this tier, and the terms
  \textit{base data} or \textit{corpus} for its data.

  \item The query processing tier is responsible for providing the functionality to identify and retrieve data using
  queries on secondary attributes (Section~\ref{subsec:query_prcessing_tier}).
  We use the terms \textit{query engine} or \textit{query (processing) system} for the system that implements this tier.
\end{itemize}

Disaggregating query processing from the storage engine is an approach used by various systems and cloud
services such as Amazon Athena \cite{aws:athena}, Aurora \cite{aws:aurora}, and Google BigQuery
\cite{google:bigquery}.
In these systems, query processing is performed by a query engine independent from the storage system.

This model provides several benefits:
\begin{itemize}
  \item Storage and query processing resources can scale independently (elasticity).
  \item It enables ad-hoc, one-time queries on already existing data without the need to migrate data.
  For example, performing log forensic queries to investigate an incident.
  \item It enabling cloud providers to implement fine-grained pricing for querying services.
\end{itemize}

In this work, we focus on the design of the query processing tier and consider the data storage tier as ``imposed'':
we consider the data storage tier's functionalities, guarantees, and data distribution schemes as input parameters in
our design.

\bigskip

\noindent
\textbf{Data and control flow} \\

\noindent
Data and control flow between users and the system's tiers as follows: (TODO: need figure here.)
\begin{itemize}
  \item A user submits a corpus data update to the data storage tier.
  As a result, data flows from the user to the data storage tier, and then optionally to query processing tier.
  The query processing tier may use this data to update data structures such as indexes or materialized views.

  \item A user submits a query to the query processing tier.
  As a result, control messages linked to the query execution flow through the query processing tier, and potentially
  for the query processing to the data storage tier (a query processed using cached data does not require control flow
  between tiers, while a query that requires reading from base tables does).
  Data --- in the form of intermediate or final query results flow from the storage tier to the query processing tier to
  the user.
\end{itemize}

The bidirectional flow of data and control messages the fundamental property of the presented system model, and guides
our query processing tier design, as discussed in section TODO.

\subsection{Data storage tier}

The data storage tier is a broad abstraction that can include any system that can be used to store and retrieve data.
This can include databases, file systems, cloud object storage systems.

As described above, in this work we view this tier as an external, underlying system that our design can build upon.
However, any efficient query processing tier design needs to take into account the properties and characteristics of the
data storage tier, most importantly its data distribution scheme.
To address this, we adopt a narrower data storage tier definition.
More specifically, we model a data storage tier as a federation of geo-replicated storage systems.
In the following section, we present our data storage tier model in detail.


\subsubsection{System model}
The data storage tier is implemented by a storage system (a database, file system or cloud storage system) or a
federation of multiple, potentially heterogeneous storage systems.
Each system is responsible for storing a large corpus dataset $D$, which is updated by a stream of small changes.
In a federated system, the dataset of each storage system is independent, but the application logic may implement
replication across different systems (for example an application that replicates data across multiple cloud providers).
In addition, storage systems implement replication:
each storage system maintains one or more full replicas of its dataset, and may also maintain partial replicas.
Finally, storage systems may employ sharding:
each replica partitions its data in non-overlapping parts, and a shard is responsible
for a part of the data.
Replication and sharding are independent, orthogonal techniques, but are often both used in large-scale systems.

We model the infrastructure on which the data storage tier runs as a collection of \textit{sites}.
A site is a group of nodes (servers, user devices) with the following characteristics:
\begin{itemize}
  \item Network communication latency between nodes in different sites is significantly higher (typically an order of
  magnitude higher) \cite{pbailis:hats} compared to communication latency within a site.
  \item Network bandwidth is high within a site and more limited and costly across sites.
  This is reflected in the pricing for cross-region data transfer in public cloud platforms.
  Using the AWS Pricing Calculator \cite{aws:costcalc} we see that data transfer to different AWS data centers
  (regions) costs double the price of intra-DC data transfer (0.02 and 0.01 USD per GB respectively).
\end{itemize}

A site can correspond to a data center, a group of servers serving as an edge Point of Presence \cite{google:infra}, or
a group of user devices in close proximity (in the same room or building).


\subsubsection{Data model}
The corpus dataset is a collection of \textit{data items}, organized in \textit{tables}.
We use the term \textit{data item} to refer to the unit of stored data:
Depending on the storage system, a data item may correspond to a file, an object, or a database record.
Tables may be organized in a hierarchical structure (file system directories), a flat namespace (object store buckets),
or in a relational schema.

A data item is composed of a primary key and a set of attributes.
The primary key can be used to efficiently identify and retrieve the data item, without requiring a scan.
Attributes are key-value pairs: each data item is associated with a map of attribute keys and values
$\{AttrKey: AttrVal\}$.

Our main focus is on data item attributes: we consider that updates and queries refer to attributes.
We do not assume a strict schema for attributes: the attributes of each data item are independent of the attributes of
others.
Finally, the data item may contain a value (content), such a an image, video, or PDF file.
We treat this content as one of the data item's attributes.
(
TODO: should we differentiate it? having the content as a normal attribute creates the assumption that it can be part of
query processing as any other attribute. We haven't considered in our design consideration of image / video / ... search.)

This data model can express the data model of multiple different types of storage systems:
\begin{itemize}
  \item Object stores, such as AWS S3: \\
  The data storage tier is implemented as an object store.
  In that case, a data item corresponds to an object and a tables to a bucket.
  In addition, a data item's ``content'' attribute corresponds to the object's content, and the rest of the attributes
  correspond to object tags \cite{awss3:tagging}.

  \item Wide-column stores, such as Amazon DynamoDB, Google's Bigtable or Apache Cassandra: \\
  In a data storage tier implemented by a wide-column store:
  \begin{itemize}
    \item Data is organized in tables.
    \item Data items correspond to table rows and attributes correspond to columns.
  \end{itemize}
  \item Relational databases: \\
  In a data storage tier implemented by a relational database each table record corresponds to a data item, and each
  table column corresponds to an attribute.
  The relational schema can be represented by enforcing a corresponding schema for the attributes of data items in each
  table.

  \item Document-oriented databases, such as MongoDB: \\
  Document-oriented databases (or document stores) are one of the categories of the group of databases termed NoSQL
  databases.
  The are designed to manage information in the form of documents (semi-structured) data.
  Documents encode data in some standard format such as XML, YAML, JSON or BSON.
  TODO: now introduced in the background chapter. Maybe remove general description from here.

  The described data model is partially compatible with the document store data model:
  tables correspond to document collections, and data items correspond to documents.
  A document's identifier can be represented as a data item's primary key, while document attributes can be represented
  as attributes.
  However, the described data model cannot express complex attribute types such as lists and maps, and nested attributed,
  which are used in the document formats used in document stores.

  \item File systems: \\
  In a data storage tier implemented by a file system, data items correspond to files and tables correspond to file
  system directories.
  A data item's primary key corresponds to the corresponding file's path, the ``content'' attribute to the file's content,
  and rest of attributes correspond to extended file attributes.
\end{itemize}

This general data model, which is able to express different existing data models, satisfies the design goal mentioned
above:
Allowing the query processing tier to be independent from the underlying data storage tier, and be compatible with
different storage tier implementations. \\

\noindent \textbf{Timestamps} \\
We assume that objects are ``versioned''.
Each object is associated with a timestamp, which is one of its attributes.
A timestamp can be implemented by any data type that provides a partial of total order,
such as Unix time or vector timestamps.


\subsubsection{Data storage tier API}
One of our design goals
(TODO: link to design goals in the next chapter)
is that the query processing tier should be agnostic of the data storage tier.
A query system should be able to interoperate with multiple different data storage tier implementations.
To achieve that, we model the interconnection between the data storage and query processing tier as a set of well-defined APIs.
This allows the query processing tier to be agnostic of the data storage tier, and compatible with any storage system
that exposes the described APIs.

% As an example, the query processing tier can work on top of a streaming system that does not persist data but exposes the $Subscribe$ API.

To be compatible with our design, a data storage tier need to expose some the following APIs:
\begin{itemize}
  \item An API for iterating over the corpus data (List).
  This API enables the query processing tier to access corpus data.
  It can be used to implement query processing tasks such as iterating over a table's data items and filtering those that
  match a given predicate, or performing a join over two tables.
  \item An API for subscribing to notification for changes to the corpus data (Subscribe).
  This API enables the query processing tier to receive a constant stream of notifications for corpus data changes,
  which can be used to incrementally maintain data structures such as indexes and materialized views.
  \item An API for querying the corpus data (Select).
  This API allow the query processing tier to make use of the querying capabilities of the data storage tier.
\end{itemize}

List and Subscribe are complementary, and are both needed for a fully featured query processing tier.
Select is operational, as it can be viewed as a generalization of List (a List is a Select without predicates)
Our query processing tier design is compatible with data storage systems that expose only one of the described APIs,
albeit with some limitations.

In short:
\begin{itemize}
  \item When only the List API is available, the query processing tier can perform any querying task, but the use of
  derived data for optimizations is limited.
  The query processing tier can build indexes and materialized views by scanning the corpus data, but updating those data
  structures to reflect corpus data changes requires a full rebuild.

  \item When only the List API is available, the query processing tier can perform query processing tasks only by
  building a derived view of the corpus data (build materialized views).
  Depending on the semantics of Subscribe, the query processing tier may only be able to receive information for corpus
  updates with timestamps starting from the invocation of the Subscribe API, making the query processing tier not
  possible to deploy over already existing data.
\end{itemize}

In chapter
TODO,
we discuss the implications of the data storage tier exposing only one of the APIs to the query processing tier
functionality.
In addition we discuss the implications of $List$ and $Subscribe$ not supporting a range of timestamps but rather
returning results about the most recent timestamp, and how the query processing tier can make use of the $Select$ API.
For the rest of this document, we assume, unless stated otherwise, that the data storage tier exposes the $List$ and
$Subscribe$ APIs.

\bigskip

\noindent
\textbf{API 1: List}

\noindent
This API provides a mechanism for retrieving the primary key and attributes of all objects in a given table:

\[
  List(Table, [Timestamp_{low}, Timestamp_{high})) \rightarrow [ListResponse]
\]

\noindent
\begin{sloppypar}
Given a table name ($Table$) and a range of timestamps ($[Timestamp_{low}, \linebreak Timestamp_{high})$),
$ListResponse$ contains all data items in $Table$ with $Timestamp_{low} \leq Timestamp < Timestamp_{high}$.
Each data item in $ListResponse$ is represented as a tuple $(ID, \{AttrKey: AttrVal\})$, containing the data item's
primary key ($ID$), and attributes ($\{AttrKey: AttrVal\}$).
\end{sloppypar}

$ListResponse$ may be implemented in different ways, such a single response containing a set of
data items, or a stream in which each data item is sent as a record, or an iterator in which calling a $Next()$ method
returns the following data item.
Finally, we don't assume any ordering in $ListResponse$.

The $List$ API may be provided as an explicit API method or implemented as a combination of a \textit{list} and a
\textit{read} (get) operation.

Depending on its versioning mechanism, the storage system implementing the data storage tier may not support the
listing API for any range of timestamps.
For example, in a storage system that does not provide multi-versioned storage, $List$ will return the latest version of
each data item.
For simplicity, we assume the $List$ API as specified above.
In Section
TODO
we describe how our design can use the $List$ API without support for Timestamp ranges.

\bigskip

\noindent
\textbf{API 2: Subscribe}

\noindent
This API provides a mechanism for subscribing to notification for updates to data items in a given table:

\[
  Subscribe(Table, [Timestamp_{low}, Timestamp_{high})) \rightarrow [SubscribeResponse]
\]

\noindent
\begin{sloppypar}
Given a table name ($Table$) and a range of timestamps ($[Timestamp_{low}, Timestamp_{high})$),
$SubscribeResponse$ contains a record for each update performed in a data item $d$ in $Table$, with
$Timestamp_{low} \leq Timestamp < Timestamp_{high}$.
$SubscribeResponse$ records have the form $(ID, \{AttrKey: (AttrVal_{old}, AttrVal_{new})\})$:
each record contains the update data item's attributes before and after the update.
If an operation updates more than one data item, then $SubscribeResponse$ contains a record for each updated data item.
If an attribute is created by the update, then its old value has the special value $null$.
Conversely, if an attribute is deleted by the update, then its new value has the $null$ value.
\end{sloppypar}

$SubscribeResponse$ can be implemented as a stream:
An invocation of $Subscribe$ returns a stream handler, and a record is sent to stream for each corresponding update.

Various systems provide mechanisms that can be used to implement the $Subscribe$ API.
Examples include triggers in traditional database management systems \cite{mariadb:triggers}, and event notification
mechanisms in cloud storage services \cite{awss3:notifications}.

\bigskip

\noindent
\textbf{API 3: Select}

\noindent
This API provides a mechanism for identifying data items based on their attributes:

\begin{sloppypar}
\[
  Select(Table, Predicate, [Timestamp_{low}, Timestamp_{high})) \rightarrow [SelectResponse],
\]
where $Predicate$ is a map containing attribute keys and ranges:
$\{AttrKey: [AttrVal_{low}, AttrVal_{high})\}$.
\end{sloppypar}

\begin{sloppypar}
Given a table name ($Table$), a range of timestamps ($[Timestamp_{low}, Timestamp_{high})$), and
a predicate consisting of attributes keys and ranges of values
($\{AttrKey: [AttrVal_{low}, AttrVal_{high})\}$), $Select$ returns $SelectResponse$.
$SelectResponse$ contains all data items in $Table$ that have all attributes contained in $Predicate$ and
their values are within the ranges specified in $Predicate$.
\end{sloppypar}

$Select$ can be expressed as an SQL query with the form: \\
\noindent
$SELECT$ $AttrKey_1$, $AttrKey_2$, ..., \\
$FROM$ $Table$ \\
$WHERE$ \\
$AttrKey_1$ $IS$ $NOT$ $NULL$ $AND$ $AttrVal_{low}$ $\leq$ $AttrKey_1$ $<$ $AttrVal_{high}$
$AND$ $AttrKey_2$ $IS$ $NOT$ $NULL$ $AND$ $AttrVal_{low}$ $\leq$ $AttrKey_2$ $<$ $AttrVal_{high}$
$AND$ ... \\
$AND$ $Timestamp_{low}$ $\leq$ $Timestamp$ $<$ $Timestamp_{high}$

$Select$ may be provided as a subset of a more expressive query language, such as in relational database
systems that support a full SQL query language. \\

\subsection{Query processing tier}
\label{subsec:query_prcessing_tier}
The design decisions and trade-offs involved in the design of the query processing tier is the main focus of this work.
In this Section we present an overview of the query processing tier's role and functionality.
The following chapters present our query processing tier design in more detail.

The query processing tier is responsible for providing attribute-based data retrieval:
Identifying and retrieving data items based on queries on their attributes.

As the query language, we consider the subset of SQL that supports expressions of the form: \\

\noindent
$SELECT$ $projection$ $FROM$ $Table$ $WHERE$ $predicate$,

\noindent
where
% \begin{itemize}
%   \item $projection$ is a list of attributes, $AttrKey_1$, $AttrKey_2$, ..., $AttrKey_N$.
%   \item $predicate$: has the form: \\ $rangePredicate_1$ $AND$ $rangePredicate_2$ $AND$ ... $AND$ $rangePredicate_N$, \\
%   where $rangePredicate_i$ an expression of the form $AttrVal_{low}$ $\leq$ $AttrKey$ $<$ $AttrVal_{high}$.
% \end{itemize}

TODO: this needs some work to define exactly what is and is not supported (we now support more than previous iterations, for example some simple joins).

Given a query $Q$ = ($projection$, $table$, $predicate$)
and a data item $d$ = ($table$, $id$, $attributes$), $d$ satisfies $Q$ if:
\begin{itemize}
  \item $Q.table$ = $d.table$: the data item belongs in the table referred by the query, and
  \item $\forall$ ($AttrVal_{low}$ $\leq$ $AttrKey$ $<$ $AttrVal_{high}$) $\in$ $Q.predicate$:
  $attrKey$ $\in$ $d.attributes$ and $AttrVal_{low}$ $\leq$ $AttrVal$ $<$ $AttrVal_{high}$:
  the data item contains all attributes included in the query's predicate, and their values are within the ranges
  specified by the predicate.
\end{itemize}

$Predicate$ may contain expressions that refer to the $Timestamp$ predicate.
In that case, a data item satisfies $Q$ if, in addition to the above conditions, its $Timestamp$ satisfies the
$Timestamp$ -related expressions in $predicate$.
If no $Timestamp$ -related predication is given, then only the latest version of each data item is considered for the
query.

We define the set of data items that satisfy a query $Q$ as $Q_r$.
In the ideal case, $Q$'s response is $Q_r$, however, as we explain in the following section a query response may
differ from $Q_r$. \\

\noindent
\textbf{Derived data} \\
Query systems often maintain data such as indexes, materialized views, and caches.
We refer to this data as \textit{derived} data.
Derived data, in the general case, is obtained by performing a computation over the base data.
When base data changes, the query system need to update derived data to reflect theses changes.
After a base data updates, indexes and materialized views need to be update so that the reflect the new state of the corpus,
and obsolete cache entries need to be invalidated.
As we describe in the following section, inconsistencies between base and derived data may result to divergence between
query responses and $Q_r$, as $Q_r$ is defined using the most up-to-date state of the corpus at the time of a query, but
query processing may be based on an outdated view of the corpus data.

\section{Query processing system performance evaluation}
\label{sec:requirements}

% An important first step in the process of designing a query processing system (and any system in general) is determining
% the factors and metrics that will be used to evaluate how well the design achieves its goals.

The aspects of a query processing system's performance can be categorized in two groups:
\textit{efficiency} and \textit{effectiveness} \cite{buttcher:informationretrieval}.
We can measure efficiency with metrics such as response time, throughput, and scalability.
Effectiveness is a measure of how well a query processing system achieves its intended purpose.
It involves metrics such as precision (the fraction of useful information returned by the query) and recall
(the fraction of data items in the corpus that satisfy a query returned by the query).

Finally, two other important factors in the design of query processing systems are availability and operational cost.
Availability is important, due to the negative effects of downtime in user serving systems.
It is especially relevant in the design of distributed systems due to the many different faults that can impact the
operation of a distributed system \cite{kleppmann:designing}.

\todo{Cost ...}

\subsection{Evaluating Efficiency}

The most visible aspect of efficiency is the \textit{response time} experienced by a user between issuing a query and
receiving the corresponding response.
Since the query system may need to support many simultaneous users, \textit{query throughput}, measured in queries served
per unit of time, is an important performance metric for the query processing system.
In addition, as response time is the measure that directly affects user experience, an important efficiency metric is
how response time scales with query throughput.
This measures whether the system can maintain low query response time for individual users as the number of users
increases.
The relations between response time and query throughput, and between throughput and system load (number of users),
characterize the system's \textit{scalability}.

\subsubsection{Response time}
Response time --- the amount of time between making a request and receiving the corresponding response ---
is among the most important metrics for the quality of a user-facing service.

A number of studies and experiments have studied the effects of response time to user experience.
Results show that response time is among the factors that have the largest effect users' subjective perception of the
quality of a system.
Users have been shown to perceive websites that load faster as more interesting \cite{ramsay/retrievaltimesinvestigation}.
On the other hand, long response times increase user frustration \cite{ceaparu:userfrustration} and even compromise
user's conceptions of the security of the system \cite{bouch:qualityeyebeholder}.

Industry reports have indicated that even small increases in user-perceived response times can result in drops in web
traffic, and therefore sales.
Experiments by the Google and Bing search engines have shown that longer page loading times have a significant impact on
metrics such as time to click, repeated site usage, and queries per visit \cite{schurman:rerformanceuserimpact}.
A study from Akamai on the impact of travel site performance on consumers showed that more than half of the users will
wait three seconds or less before abandoning the site \cite{akamai:travelsiteperformance}.
Finally, a comparison shopping service (Shopzilla) has reported that a website re-engineering project that achieved a
speedup in page load time from 6-9 seconds down to 1.2 seconds resulted in 25\% increase in page views and 5-12\%
increase in revenue \cite{dixon:shopzillasiteredo}.

\subsection{Evaluating Effectiveness}

Effectiveness is a measure of how well a query processing system achieves its intended purpose.

\subsubsection{Recall and precision}

In the field of information retrieval, which covers the problems associated with searching human-language data,
the key notion linked with effectiveness is \textit{relevance} \cite{buttcher:informationretrieval}.
In information retrieval, given a user's information need, represented by a search query, the \textit{search engine}
(the system responsible for query processing) computes a relevance \textit{score} for each document (e-mail message,
webpage, news article), and returns a ranked list of results.

Recall and precision are metrics often used to measure the query results' relevance:
\begin{itemize}
  \item Recall is the fraction of the relevant that are returned by the query.
  A recall value equal to 1 indicates that all relevant documents are returned by the query
  A recall value of less that 1 indicates that some relevant documents are not returned (``false-negatives'').

  \item Precision is the fraction of relevant documents among the documents contained in the query result.
  A precision value equal to 1 indicates that all documents returned by the query are relevant.
  A precision value of less than 1 indicates that some of the returned documents are not relevant (``false-positives'').
\end{itemize}

The difference between the information retrieval query model, and the query model that we consider in this work is the
notion of relevance.
In information retrieval, relevance is a spectrum: documents can be more or less relevant to a given query, which
represented by their \textit{score} for that query.
Here, relevance is binary: a data item is either relevant (satisfies the given query) or it is not.
However, as mentioned in Section \ref{subsec:query_prcessing_tier}, similarly to information retrieval, query results
can include non-relevant data items (false positives), or not include relevant data items (false-negatives).
Therefore we argue that the recall and precision are useful metrics in evaluating the effectiveness of the query
processing tier.

\subsubsection{Freshness}

As mentioned in Section \ref{subsec:query_prcessing_tier}, inconsistencies between base and derived data can lower query
processing effectiveness by introduction false-positives and false-negatives.

Traditional database systems often keep derived data consistent with base data by updating both in a single transaction.
For example, when executing an $UPDATE$ statement, MariaDB updates a table's secondary indexes in the same transaction
as the table rows \cite{innodb:writepaths}.
However, in systems that implement asynchronous (lazy) derived data maintenance policies \cite{tan:diffindex,
qi:secondaryindexconsistency, shukla:schemaagnostic} derived data can become stale with respect to base data.

Stale derived data may introduce false-positives and false-negatives to query results:
\begin{itemize}
  \item \textbf{False-positives}: A data item $d$ has been deleted (or updated so that it matches the given query),
  but the corresponding derived data has not yet been updated to reflect this change.
  A query execution that uses the stale derived data will include $d$ in the query response, introducing a false-positive.

  \item \textbf{False-negatives}: A data item $d$ has been created (or updated so that it does not match the given query),
  but the corresponding derived data has not yet been updated to reflect this change.
  A query execution that uses the stale derived data will not include $d$ in the query response,
  introducing a false-negative.
\end{itemize}
This can result in false-positives and false-negatives, affecting the recall and precision of these systems.

We use the notion of \textit{freshness} to refer to the measure of consistency between base and derived data due to
asynchronous derived data updates.

A number of metrics for measuring data freshness have been proposed in the literature \cite{bouzeghoub:datafreshness}:
\begin{itemize}
  \item \textbf{Currency} measures the time between a change in the source data, and that change between reflected in
  the derived data.
  In caching systems, the terms recency \cite{bright:latencyrecency} and age \cite{cho:dbfreshness}
  have been used to describe this metric.
  \item \textbf{Obsolescence} measures the number of updates to source data since derived data was last updated.
  Work on query systems has defined the \textit{obsolescence cost} \cite{avigdor:obsolescent}, of a query to represent
  the penalty of basing a query result on obsolescent materialized view.
  This cost is computed as a function of the number of insertions, updates, and deletions that cause deviation between
  the materialized view and and the base table.
  \item \textbf{Freshness-rate} measures the percentage of derived data entries that are up-to-date with the source
  data.
  This metric has been used to quantify the freshness of web pages \cite{labrinidis:balancingperfomancefreshness} and
  local databases copies \cite{cho:dbfreshness}.
\end{itemize}

\subsection{Other aspects of query processing system design}

\subsubsection{Availability}
The importance of availability becomes apparent when considering the negative effects of service downtime.

A study on user behavior in the Web \cite{nah:waitingtime} found that users abandon a nonworking hyperlink after
5-8 seconds.

Operators of global services understand that ``even slightest outage has significant financial
consequences and impacts customer trust'' \cite{deCandia:dynamo}.
A service 49 minute service outage in January 2013 cost Amazon an estimated \$4 million or more in lost sales
\cite{infoworld:cloudoutages}.

\subsubsection{Operational Cost}
% Operating a query processing system requires computation, memory, network and storage resources.
Another important parameter that drives system design parameters is the system's operational cost.

Traditionally, the system is deployed on dedicated infrastructure, and the operational cost is the cost of owning and
operating that infrastructure.

More recently, the infrastructure and platform as a service cloud computing models providing flexible, fine-grained
provisioning of computing, storage and networking resources.
In addition, these services provide fined-grained, ``pay-as-you-go'' pricing mechanisms.
As a result, system designers can have more control over the system's operational cost.

A typical cloud pricing model \cite{aws:pricing} has distinct pricing for (1) computation and memory resources
(vCPUs and memory), (2) persistent storage, and (3) data transfer.

In chapter
TODO
we describe how system design decisions can affect the system's resource utilization, for example limiting the cross-DC
communication, and therefore its operational cost.

\bibliographystyle{plainnat}
\bibliography{refs}